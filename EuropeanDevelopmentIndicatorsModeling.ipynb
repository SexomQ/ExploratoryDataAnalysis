{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf7fffa",
   "metadata": {},
   "source": [
    "# Modeling and Regression Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9aa4c4",
   "metadata": {},
   "source": [
    "## Model Selection & Feature Engineering\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6361d09",
   "metadata": {},
   "source": [
    "### Variable Selection\n",
    "\n",
    "1. GDP Growth Rate (gdp_growth)\n",
    " - Unit: Percentage (%), Quarter-over-Quarter\n",
    "\n",
    "2. Inflation Rate (inflation)\n",
    " - Unit: Percentage (%), year-over-year growth rate\n",
    "\n",
    "3. Unemployment Rate (unemployment)\n",
    " - Unit: Percentage (%)\n",
    " - Justification:\n",
    "    - Labor market health indicator - measures human capital utilization\n",
    "    - High policy relevance - directly impacts household income and social welfare\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae8779a",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "**1. GDP Growth Predictors**\n",
    "\n",
    "**Selected Features:**\n",
    "- `gdp_growth_lag1`, `gdp_growth_lag4` - Momentum and seasonal patterns\n",
    "- `inflation_lag1` - Real demand effects\n",
    "- `unemployment_lag1` - Labor utilization (Okun's Law)\n",
    "- `exports_lag1`, `trade_balance_lag1` - External sector contribution (lagged)\n",
    "- `consumption_lag1` - Largest GDP component (~70-80%, lagged)\n",
    "- `import_ratio_lag1` - Domestic demand proxy (lagged)\n",
    "\n",
    "**Economic Rationale:**\n",
    "- **Lagged GDP:** EDA shows stable, predictable expansion with near-normal distribution → strong autoregressive patterns expected\n",
    "- **Inflation (lagged):** Well-anchored around 2.3% but weak correlations to other indicators (r = -0.27 to 0.14) → operates independently, driven by external factors in small, open economies\n",
    "- **Unemployment (lagged):** EDA reveals paradoxical positive GDP-unemployment correlation (r = 0.77) → 'jobless growth' pattern where productivity gains don't convert to employment\n",
    "- **Trade variables (lagged):** Critical for small, open economies highly integrated into European markets\n",
    "\n",
    "**Forecasting Assumption:** All predictors use lagged values (t-1, t-4), reflecting pure out-of-sample forecasting where only historical data is available at prediction time.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Inflation Predictors**\n",
    "\n",
    "**Selected Features:**\n",
    "- `inflation_lag1`, `inflation_lag4`, `inflation_ma4` - Persistence and trend\n",
    "- `gdp_growth_lag1` - Demand-pull inflation (output gap, lagged)\n",
    "- `unemployment_lag1` - Phillips Curve relationship (lagged)\n",
    "- `imports_lag1`, `import_ratio_lag1`, `trade_balance_lag1` - Import price pass-through (lagged)\n",
    "\n",
    "**Economic Rationale:**\n",
    "- **Lagged inflation:** Strong persistence. EDA shows range from -1.6% to 10.3% (COVID spike) with crisis-driven disruptions. Year-over-year lag (t-4) removes seasonality\n",
    "- **GDP growth (lagged):** Limited correlation in Luxembourg → external inflation drivers dominate domestic demand-pull. One-quarter lag captures delayed transmission of demand pressures to prices\n",
    "- **Unemployment (lagged):** EDA (2004-2024) confirms negative Phillips Curve correlation, but relationship breaks down during major shocks (2008, COVID-19). Lag reflects delayed wage-price dynamics\n",
    "- **Trade variables (lagged):** Given weak domestic correlations, external trade-transmitted inflation is dominant channel. Import price changes transmit to domestic inflation with lag\n",
    "- **Moving average:** Captures underlying trend vs. temporary crisis shocks\n",
    "\n",
    "**Forecasting Assumption:** All predictors lagged (t-1, t-4) to ensure realistic forecasting scenario where future quarter's inflation predicted using only historical data.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Unemployment Predictors**\n",
    "\n",
    "**Selected Features:**\n",
    "- `unemployment_lag1`, `unemployment_lag4` - Persistence and seasonal adjustment\n",
    "- `gdp_growth_lag1`, `gdp_growth_lag4` - Okun's Law (lagged effects)\n",
    "- `inflation_lag1` - Phillips Curve trade-off (lagged)\n",
    "- `emigration_lag1`, `net_migration_lag1` - Labor supply dynamics (lagged)\n",
    "- `exports_lag1`, `consumption_lag1` - Job creation channels (lagged)\n",
    "\n",
    "**Economic Rationale:**\n",
    "- **Lagged unemployment:** EDA reveals bimodal distribution (peaks at 2-3% and 5-7%) → two distinct labor market regimes. Strong persistence captured by quarterly and year-over-year lags\n",
    "- **GDP growth (lagged):** Despite Okun's Law predicting negative correlation, EDA shows positive correlation (r = 0.77) → 'jobless growth' driven by capital intensity and automation. Lagged GDP reflects delayed employment adjustment to output changes\n",
    "- **Inflation (lagged):** Phillips Curve confirmed but temporally variable. Lag reflects delayed transmission from price changes to wage adjustments and labor market outcomes\n",
    "- **Migration (lagged):** Luxembourg-specific. Population grew 2.14x; immigration (17k/year) >> emigration (10k/year). **Critical finding:** Inverted unemployment-emigration relationship → during downturns, emigration decreases (alternative destinations also suffer)\n",
    "- **Demand indicators (lagged):** Export sectors vs. domestic service sectors respond differently. Employment creation follows demand with lag as firms adjust hiring decisions\n",
    "\n",
    "**Forecasting Assumption:** All predictors lagged (t-1, t-4) reflecting realistic forecasting where unemployment in quarter t predicted using data through quarter t-1. Captures delayed labor market adjustments to economic conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6354067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_modeling_features(df):\n",
    "    \"\"\"\n",
    "    Create comprehensive feature set for GDP Growth, Inflation, and Unemployment modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a copy to avoid modifying original\n",
    "    data = df.copy()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"FEATURE ENGINEERING FOR MODELING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 1. TARGET VARIABLES\n",
    "    # ========================================================================\n",
    "    print(\"\\n1. Creating Target Variables...\")\n",
    "    \n",
    "    # GDP Growth Rate (Quarter-over-Quarter, %)\n",
    "    data['gdp_growth'] = data['gdp'].pct_change() * 100\n",
    "    print(\"   ✓ gdp_growth (QoQ %)\")\n",
    "    \n",
    "    # GDP Growth Rate (Year-over-Year, %)\n",
    "    data['gdp_growth_yoy'] = data['gdp'].pct_change(periods=4) * 100\n",
    "    print(\"   ✓ gdp_growth_yoy (YoY %)\")\n",
    "    \n",
    "    # Inflation rate (already in dataset)\n",
    "    # Unemployment rate (already in dataset)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 2. DERIVED ECONOMIC INDICATORS (BEFORE LAGGING)\n",
    "    # ========================================================================\n",
    "    print(\"\\n2. Creating Derived Economic Indicators...\")\n",
    "    \n",
    "    # Trade indicators\n",
    "    data['trade_balance'] = data['exports'] - data['imports']\n",
    "    data['trade_openness'] = (data['exports'] + data['imports']) / data['gdp']\n",
    "    data['export_ratio'] = data['exports'] / data['gdp']\n",
    "    data['import_ratio'] = data['imports'] / data['gdp']\n",
    "    print(\"   ✓ trade_balance, trade_openness, export_ratio, import_ratio\")\n",
    "    \n",
    "    # Per capita indicators\n",
    "    data['gdp_per_capita'] = (data['gdp'] * 1_000_000) / data['population']\n",
    "    data['consumption_per_capita'] = (data['consumption'] * 1_000_000) / data['population']\n",
    "    print(\"   ✓ gdp_per_capita, consumption_per_capita\")\n",
    "    \n",
    "    # Migration indicators\n",
    "    data['net_migration'] = data['immigration'] - data['emigration']\n",
    "    data['net_migration_rate'] = (data['net_migration'] / data['population']) * 1000\n",
    "    data['emigration_rate'] = (data['emigration'] / data['population']) * 1000\n",
    "    data['immigration_rate'] = (data['immigration'] / data['population']) * 1000\n",
    "    print(\"   ✓ net_migration, net_migration_rate, emigration_rate, immigration_rate\")\n",
    "    \n",
    "    # Demographic indicators\n",
    "    data['deaths_rate'] = (data['deaths'] / data['population']) * 1000\n",
    "    print(\"   ✓ deaths_rate\")\n",
    "    \n",
    "    # Current account as % of GDP\n",
    "    data['ca_to_gdp'] = (data['acc_balance'] / data['gdp']) * 100\n",
    "    print(\"   ✓ ca_to_gdp\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 3. LAGGED VARIABLES (for time series dependencies)\n",
    "    # ========================================================================\n",
    "    print(\"\\n3. Creating Lagged Variables...\")\n",
    "    \n",
    "    # Variables to lag - IMPORTANT: Create derived features FIRST, then lag them\n",
    "    lag_vars = [\n",
    "        # Target and base variables\n",
    "        'gdp_growth',           \n",
    "        'inflation',            \n",
    "        'unemployment',         \n",
    "        # Base economic variables\n",
    "        'exports',              \n",
    "        'imports',              \n",
    "        'consumption',          \n",
    "        'emigration',           \n",
    "        'net_migration',        \n",
    "        # Derived ratios/indicators\n",
    "        'trade_balance',        \n",
    "        'import_ratio',  \n",
    "        'export_ratio',         \n",
    "        'trade_openness'        \n",
    "    ]\n",
    "    \n",
    "    for var in lag_vars:\n",
    "        if var in data.columns:\n",
    "            # Lag 1 (previous quarter)\n",
    "            data[f'{var}_lag1'] = data[var].shift(1)\n",
    "            # Lag 4 (year ago - removes seasonality)\n",
    "            data[f'{var}_lag4'] = data[var].shift(4)\n",
    "            print(f\"   ✓ {var}_lag1, {var}_lag4\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  Warning: '{var}' not found, skipping lags\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 4. MOVING AVERAGES (for smoothing volatility)\n",
    "    # ========================================================================\n",
    "    print(\"\\n4. Creating Moving Averages...\")\n",
    "    \n",
    "    # Variables to smooth\n",
    "    ma_vars = [\n",
    "        'inflation',            # Smooth inflation volatility\n",
    "        'unemployment',         # Smooth labor market fluctuations\n",
    "        'gdp_growth'           # Smooth growth volatility\n",
    "    ]\n",
    "    \n",
    "    for var in ma_vars:\n",
    "        if var in data.columns:\n",
    "            # 2-quarter moving average\n",
    "            data[f'{var}_ma2'] = data[var].rolling(window=2, min_periods=1).mean()\n",
    "            # 4-quarter (annual) moving average\n",
    "            data[f'{var}_ma4'] = data[var].rolling(window=4, min_periods=1).mean()\n",
    "            print(f\"   ✓ {var}_ma2, {var}_ma4\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 5. FEATURE SUMMARY\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Count features by category\n",
    "    base_features = ['Date', 'inflation', 'unemployment', 'gdp', 'exports', 'imports', \n",
    "                     'acc_balance', 'consumption', 'population', 'emigration', \n",
    "                     'immigration', 'life_expectancy', 'deaths']\n",
    "    \n",
    "    target_features = [col for col in data.columns if 'gdp_growth' in col]\n",
    "    derived_features = [col for col in data.columns if any(x in col for x in \n",
    "                       ['trade_', 'per_capita', 'migration', 'deaths_rate', 'ca_to_gdp', \n",
    "                        '_ratio', 'openness'])]\n",
    "    lag_features = [col for col in data.columns if '_lag' in col]\n",
    "    ma_features = [col for col in data.columns if '_ma' in col]\n",
    "    \n",
    "    print(f\"\\nBase indicators:        {len([c for c in base_features if c in data.columns])}\")\n",
    "    print(f\"Target variables:       {len(target_features)}\")\n",
    "    print(f\"Derived indicators:     {len(derived_features)}\")\n",
    "    print(f\"Lagged variables:       {len(lag_features)}\")\n",
    "    print(f\"Moving averages:        {len(ma_features)}\")\n",
    "    print(f\"\\n{'TOTAL FEATURES:':<24}{len(data.columns)}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing = data.isnull().sum().sum()\n",
    "    print(f\"\\nMissing values created: {missing}\")\n",
    "    print(f\"(Due to lagged and rolling operations - will be handled in modeling)\")\n",
    "    \n",
    "    # Verify key lagged features exist\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"VERIFICATION: Key Lagged Features\")\n",
    "    print(\"=\" * 80)\n",
    "    key_lags = ['import_ratio_lag1', 'export_ratio_lag1', 'trade_balance_lag1', \n",
    "                'consumption_lag1', 'exports_lag1', 'imports_lag1', \n",
    "                'emigration_lag1', 'net_migration_lag1']\n",
    "    \n",
    "    for lag in key_lags:\n",
    "        if lag in data.columns:\n",
    "            print(f\"   ✓ {lag}\")\n",
    "        else:\n",
    "            print(f\"   ❌ {lag} MISSING\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_predictor_sets():\n",
    "    \"\"\"\n",
    "    Return dictionary of predictor variables for each target variable.\n",
    "    Based on economic theory, EDA findings, and realistic forecasting assumptions.\n",
    "    \n",
    "    All predictors are LAGGED to ensure realistic out-of-sample forecasting.\n",
    "    \"\"\"\n",
    "    \n",
    "    predictor_sets = {\n",
    "        'gdp_growth': {\n",
    "            'base': [\n",
    "                'gdp_growth_lag1',\n",
    "                'gdp_growth_lag4',\n",
    "                'inflation_lag1',\n",
    "                'unemployment_lag1',\n",
    "                'exports_lag1',\n",
    "                'trade_balance_lag1',\n",
    "                'consumption_lag1',\n",
    "                'import_ratio_lag1'\n",
    "            ],\n",
    "            'extended': [\n",
    "                'gdp_growth_lag1',\n",
    "                'gdp_growth_lag4',\n",
    "                'inflation_lag1',\n",
    "                'inflation_ma4',\n",
    "                'unemployment_lag1',\n",
    "                'exports_lag1',\n",
    "                'imports_lag1',\n",
    "                'trade_balance_lag1',\n",
    "                'trade_openness_lag1',\n",
    "                'consumption_lag1',\n",
    "                'net_migration_rate',\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        'inflation': {\n",
    "            'base': [\n",
    "                'inflation_lag1',\n",
    "                'inflation_lag4',\n",
    "                'inflation_ma4',\n",
    "                'gdp_growth_lag1',\n",
    "                'unemployment_lag1',\n",
    "                'imports_lag1',\n",
    "                'import_ratio_lag1',\n",
    "                'trade_balance_lag1'\n",
    "            ],\n",
    "            'extended': [\n",
    "                'inflation_lag1',\n",
    "                'inflation_lag4',\n",
    "                'inflation_ma2',\n",
    "                'inflation_ma4',\n",
    "                'gdp_growth_lag1',\n",
    "                'gdp_growth_lag4',\n",
    "                'unemployment_lag1',\n",
    "                'unemployment_lag4',\n",
    "                'imports_lag1',\n",
    "                'import_ratio_lag1',\n",
    "                'trade_openness_lag1',\n",
    "                'trade_balance_lag1',\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        'unemployment': {\n",
    "            'base': [\n",
    "                'unemployment_lag1',\n",
    "                'unemployment_lag4',\n",
    "                'gdp_growth_lag1',\n",
    "                'gdp_growth_lag4',\n",
    "                'inflation_lag1',\n",
    "                'emigration_lag1',\n",
    "                'net_migration_lag1',\n",
    "                'exports_lag1',\n",
    "                'consumption_lag1'\n",
    "            ],\n",
    "            'extended': [\n",
    "                'unemployment_lag1',\n",
    "                'unemployment_lag4',\n",
    "                'unemployment_ma4',\n",
    "                'gdp_growth_lag1',\n",
    "                'gdp_growth_lag4',\n",
    "                'inflation_lag1',\n",
    "                'inflation_lag4',\n",
    "                'emigration_lag1',\n",
    "                'emigration_rate',\n",
    "                'net_migration_lag1',\n",
    "                'net_migration_rate',\n",
    "                'exports_lag1',\n",
    "                'imports_lag1',\n",
    "                'consumption_lag1',\n",
    "                'trade_balance_lag1'\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return predictor_sets\n",
    "\n",
    "\n",
    "def prepare_model_data(data, target, predictors, drop_na=True):\n",
    "    \"\"\"\n",
    "    Prepare data for modeling by selecting target and predictors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        Full feature-engineered dataset\n",
    "    target : str\n",
    "        Name of target variable\n",
    "    predictors : list\n",
    "        List of predictor variable names\n",
    "    drop_na : bool\n",
    "        Whether to drop rows with missing values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (X, y, dates) where X is predictors, y is target, dates are timestamps\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select available predictors (some may not exist)\n",
    "    available_predictors = [p for p in predictors if p in data.columns]\n",
    "    missing_predictors = [p for p in predictors if p not in data.columns]\n",
    "    \n",
    "    if missing_predictors:\n",
    "        print(f\"⚠️  Warning: Predictors not found: {missing_predictors}\")\n",
    "    \n",
    "    # Create modeling dataframe\n",
    "    model_data = data[['Date', target] + available_predictors].copy()\n",
    "    \n",
    "    if drop_na:\n",
    "        before = len(model_data)\n",
    "        model_data = model_data.dropna()\n",
    "        after = len(model_data)\n",
    "        print(f\"Dropped {before - after} rows with missing values ({after} remaining)\")\n",
    "    \n",
    "    # Separate features\n",
    "    dates = model_data['Date']\n",
    "    y = model_data[target]\n",
    "    X = model_data[available_predictors]\n",
    "    \n",
    "    return X, y, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dc63ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 119 observations from 1996-03-31 00:00:00 to 2025-09-30 00:00:00\n",
      "================================================================================\n",
      "FEATURE ENGINEERING FOR MODELING\n",
      "================================================================================\n",
      "\n",
      "1. Creating Target Variables...\n",
      "   ✓ gdp_growth (QoQ %)\n",
      "   ✓ gdp_growth_yoy (YoY %)\n",
      "\n",
      "2. Creating Derived Economic Indicators...\n",
      "   ✓ trade_balance, trade_openness, export_ratio, import_ratio\n",
      "   ✓ gdp_per_capita, consumption_per_capita\n",
      "   ✓ net_migration, net_migration_rate, emigration_rate, immigration_rate\n",
      "   ✓ deaths_rate\n",
      "   ✓ ca_to_gdp\n",
      "\n",
      "3. Creating Lagged Variables...\n",
      "   ✓ gdp_growth_lag1, gdp_growth_lag4\n",
      "   ✓ inflation_lag1, inflation_lag4\n",
      "   ✓ unemployment_lag1, unemployment_lag4\n",
      "   ✓ exports_lag1, exports_lag4\n",
      "   ✓ imports_lag1, imports_lag4\n",
      "   ✓ consumption_lag1, consumption_lag4\n",
      "   ✓ emigration_lag1, emigration_lag4\n",
      "   ✓ net_migration_lag1, net_migration_lag4\n",
      "   ✓ trade_balance_lag1, trade_balance_lag4\n",
      "   ✓ import_ratio_lag1, import_ratio_lag4\n",
      "   ✓ export_ratio_lag1, export_ratio_lag4\n",
      "   ✓ trade_openness_lag1, trade_openness_lag4\n",
      "\n",
      "4. Creating Moving Averages...\n",
      "   ✓ inflation_ma2, inflation_ma4\n",
      "   ✓ unemployment_ma2, unemployment_ma4\n",
      "   ✓ gdp_growth_ma2, gdp_growth_ma4\n",
      "\n",
      "================================================================================\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Base indicators:        13\n",
      "Target variables:       6\n",
      "Derived indicators:     26\n",
      "Lagged variables:       24\n",
      "Moving averages:        6\n",
      "\n",
      "TOTAL FEATURES:         57\n",
      "\n",
      "Missing values created: 69\n",
      "(Due to lagged and rolling operations - will be handled in modeling)\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION: Key Lagged Features\n",
      "================================================================================\n",
      "   ✓ import_ratio_lag1\n",
      "   ✓ export_ratio_lag1\n",
      "   ✓ trade_balance_lag1\n",
      "   ✓ consumption_lag1\n",
      "   ✓ exports_lag1\n",
      "   ✓ imports_lag1\n",
      "   ✓ emigration_lag1\n",
      "   ✓ net_migration_lag1\n",
      "\n",
      "✓ Saved: gen_files/modeling_data_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Create the missing features\n",
    "\n",
    "# Load the clean data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('modeling_data_clean.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "print(f\"Loaded {len(df)} observations from {df['Date'].min()} to {df['Date'].max()}\")\n",
    "\n",
    "# Create features\n",
    "df_features = create_modeling_features(df)\n",
    "\n",
    "# Save enhanced dataset\n",
    "df_features.to_csv('gen_files/modeling_data_features.csv', index=False)\n",
    "print(\"\\n✓ Saved: gen_files/modeling_data_features.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6207f3",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "**Model 1: Multiple Linear Regression (Baseline)**\n",
    "\n",
    "**Justification:**\n",
    "- **Interpretability:** Linear coefficients directly show predictor impact on target\n",
    "- **Statistical Foundation:** Provides significance tests and confidence intervals\n",
    "- **Baseline Performance:** Standard benchmark for comparison with complex models\n",
    "- **EDA Support:** Stable relationships observed (e.g., Phillips Curve, Okun's Law)\n",
    "\n",
    "---\n",
    "**Model 2: Polynomial Regression (Non-Linear)**\n",
    "\n",
    "**Justification:**\n",
    "- **Non-Linear Dynamics:** EDA reveals relationships \"break down during major shocks (2008, COVID)\"\n",
    "- **Regime Changes:** Quadratic terms capture accelerating/decelerating effects\n",
    "- **Economic Theory:** Many relationships are non-monotonic (e.g., inflation-growth inverted-U)\n",
    "- **EDA Evidence:** \"Jobless growth\" (r = 0.77), bimodal unemployment distribution → non-standard dynamics\n",
    "\n",
    "---\n",
    "\n",
    "**Model 3: HP-Filtered Regression (Cyclical Decomposition)**\n",
    "\n",
    "\n",
    "**Filtering Technique:**\n",
    "- **Hodrick-Prescott Filter** with λ = 1600 (standard for quarterly data)\n",
    "- Decomposes each variable: `Variable = Trend + Cycle`\n",
    "- Model uses **cycle components only**\n",
    "\n",
    "**Justification:**\n",
    "- **Business Cycle Focus:** EDA captures multiple cycles (2008 crisis, COVID) → isolate cyclical patterns\n",
    "- **Removes Trends:** Separates long-term growth from short-term fluctuations\n",
    "- **Stationarity:** Cyclical components more stationary → better regression assumptions\n",
    "- **Policy Relevance:** Understanding deviations from potential output (output gaps)\n",
    "---\n",
    "\n",
    "**Model 4: ARIMAX**\n",
    "\n",
    "**Components:**\n",
    "- **AR(p):** Autoregressive terms (p=1,4 for quarterly seasonality)\n",
    "- **I(d):** Integration order (d=0 if stationary, d=1 if non-stationary)\n",
    "- **MA(q):** Moving average for shock persistence (q=1,2)\n",
    "- **X:** Economic predictors as exogenous regressors\n",
    "\n",
    "**Justification:**\n",
    "- **Superior Time Series Modeling:** Explicitly captures temporal dependencies and serial correlation\n",
    "- **Automatic Stationarity:** Differencing removes trends without losing information\n",
    "- **Shock Persistence:** MA terms model how economic shocks (2008 crisis, COVID) propagate over time\n",
    "- **Economic Disruptions:** EDA shows major crisis impacts → MA component captures temporary vs. permanent effects\n",
    "- **Industry Standard:** Central banks and IMF use ARIMA family for macroeconomic forecasting\n",
    "- **Flexible Adaptation:** Order selection (p,d,q) optimized via AIC/BIC criteria\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa1407f",
   "metadata": {},
   "source": [
    "## Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd95844d",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0639f0",
   "metadata": {},
   "source": [
    "#### Data Splitting\n",
    "\n",
    "**2008-2009 Financial Crisis**\n",
    " - What: Luxembourg, with its large financial sector, was significantly impacted by the global financial crisis.\n",
    " - Outcome: The country experienced a slowdown in economic growth, prompting discussions for stronger financial regulation and increased social dimension support within the EU. \n",
    " - [link article](https://en.wikipedia.org/wiki/Luxembourg_and_the_2008_financial_crisis#:~:text=Luxembourg%20and%20the%202008%20financial,Article)\n",
    "\n",
    "**COVID-19 Pandemic (2020-2022)**\n",
    " - What: The pandemic brought widespread disruption, forcing the country into a lockdown in March 2020.\n",
    " - Outcome: The government implemented drastic measures, including school closures, border controls, and societal changes, affecting political and daily life. \n",
    " - [link article](https://today.rtl.lu/news/luxembourg/a/2285530.html#:~:text=Meanwhile%2C%20Luxembourg%20remains%20largely%20unconcerned,creative%20solutions%20during%20the%20lockdown.)\n",
    "\n",
    "Because of crises in 2008 and 2020, standard chronological splits (e.g., 80/20 at 2020) would place COVID-19 disruption entirely in the test set, biasing evaluation. Models trained only on pre-COVID data cannot learn pandemic dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b25d7e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 119 observations\n",
      "\n",
      "================================================================================\n",
      "CREATING TARGET-SPECIFIC TRAIN-TEST SPLITS\n",
      "================================================================================\n",
      "\n",
      "Total observations: 103\n",
      "Training: 88 (85.4%) - 2000 Q2 to 2021 Q4\n",
      "Test: 15 (14.6%) - 2022 Q1 to 2025 Q3\n",
      "\n",
      "================================================================================\n",
      "PREPARING DATA FOR EACH TARGET VARIABLE\n",
      "================================================================================\n",
      "\n",
      "⚠️  Note: All predictors are LAGGED (t-1, t-4) for realistic forecasting\n",
      "\n",
      "================================================================================\n",
      "TARGET: GDP_GROWTH\n",
      "================================================================================\n",
      "\n",
      "Target Variable: gdp_growth\n",
      "Predictors (8): gdp_growth_lag1, gdp_growth_lag4, inflation_lag1, unemployment_lag1, exports_lag1, trade_balance_lag1, consumption_lag1, import_ratio_lag1\n",
      "\n",
      "Training Set:\n",
      "  Before cleaning: 88 observations\n",
      "  After cleaning: 88 observations\n",
      "  Dropped: 0 (due to missing values from lags)\n",
      "\n",
      "Test Set:\n",
      "  Before cleaning: 15 observations\n",
      "  After cleaning: 15 observations\n",
      "  Dropped: 0 (due to missing values)\n",
      "\n",
      "Training Statistics for gdp_growth:\n",
      "  Mean: 1.69\n",
      "  Std: 6.29\n",
      "  Range: [-10.79, 15.32]\n",
      "\n",
      "Test Statistics for gdp_growth:\n",
      "  Mean: 0.69\n",
      "  Std: 6.40\n",
      "  Range: [-11.20, 10.79]\n",
      "\n",
      "================================================================================\n",
      "TARGET: INFLATION\n",
      "================================================================================\n",
      "\n",
      "Target Variable: inflation\n",
      "Predictors (8): inflation_lag1, inflation_lag4, inflation_ma4, gdp_growth_lag1, unemployment_lag1, imports_lag1, import_ratio_lag1, trade_balance_lag1\n",
      "\n",
      "Training Set:\n",
      "  Before cleaning: 88 observations\n",
      "  After cleaning: 88 observations\n",
      "  Dropped: 0 (due to missing values from lags)\n",
      "\n",
      "Test Set:\n",
      "  Before cleaning: 15 observations\n",
      "  After cleaning: 15 observations\n",
      "  Dropped: 0 (due to missing values)\n",
      "\n",
      "Training Statistics for inflation:\n",
      "  Mean: 2.21\n",
      "  Std: 1.47\n",
      "  Range: [-0.93, 5.67]\n",
      "\n",
      "Test Statistics for inflation:\n",
      "  Mean: 4.02\n",
      "  Std: 2.75\n",
      "  Range: [1.20, 9.47]\n",
      "\n",
      "================================================================================\n",
      "TARGET: UNEMPLOYMENT\n",
      "================================================================================\n",
      "\n",
      "Target Variable: unemployment\n",
      "Predictors (9): unemployment_lag1, unemployment_lag4, gdp_growth_lag1, gdp_growth_lag4, inflation_lag1, emigration_lag1, net_migration_lag1, exports_lag1, consumption_lag1\n",
      "\n",
      "Training Set:\n",
      "  Before cleaning: 88 observations\n",
      "  After cleaning: 88 observations\n",
      "  Dropped: 0 (due to missing values from lags)\n",
      "\n",
      "Test Set:\n",
      "  Before cleaning: 15 observations\n",
      "  After cleaning: 15 observations\n",
      "  Dropped: 0 (due to missing values)\n",
      "\n",
      "Training Statistics for unemployment:\n",
      "  Mean: 4.86\n",
      "  Std: 1.31\n",
      "  Range: [1.73, 7.33]\n",
      "\n",
      "Test Statistics for unemployment:\n",
      "  Mean: 5.60\n",
      "  Std: 0.86\n",
      "  Range: [4.23, 6.80]\n",
      "\n",
      "================================================================================\n",
      "SAVING FILES\n",
      "================================================================================\n",
      "✓ Saved: ./gen_files/train_gdp_growth.csv (88 observations)\n",
      "✓ Saved: ./gen_files/test_gdp_growth.csv (15 observations)\n",
      "✓ Saved: ./gen_files/train_inflation.csv (88 observations)\n",
      "✓ Saved: ./gen_files/test_inflation.csv (15 observations)\n",
      "✓ Saved: ./gen_files/train_unemployment.csv (88 observations)\n",
      "✓ Saved: ./gen_files/test_unemployment.csv (15 observations)\n",
      "\n",
      "✓ Saved: ./gen_files/split_summary.txt\n",
      "\n",
      "================================================================================\n",
      "DATA SPLITTING COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def create_optimal_split(df, date_col='Date', split_date='2021-12-31'):\n",
    "    \"\"\"\n",
    "    Optimal split: Train on 2000-2021 (includes both crises), Test on 2022-2025.\n",
    "    Creates separate files for each target variable.\n",
    "    \n",
    "    All predictors use LAGGED values for realistic out-of-sample forecasting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataset with Date column\n",
    "    date_col : str\n",
    "        Name of date column\n",
    "    split_date : str\n",
    "        Last date in training set (default: '2021-12-31')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing train/test splits for each target variable\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    df = df[df[date_col] >= '2000-01-01']  # Ensure data starts from 2000 Q1\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df = df.sort_values(date_col).reset_index(drop=True)\n",
    "    \n",
    "    split_dt = pd.to_datetime(split_date)\n",
    "    \n",
    "    # Create basic splits\n",
    "    train_df = df[df[date_col] <= split_dt].copy()\n",
    "    test_df = df[df[date_col] > split_dt].copy()\n",
    "    \n",
    "    n_total = len(df)\n",
    "    n_train = len(train_df)\n",
    "    n_test = len(test_df)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"CREATING TARGET-SPECIFIC TRAIN-TEST SPLITS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nTotal observations: {n_total}\")\n",
    "    print(f\"Training: {n_train} ({n_train/n_total*100:.1f}%) - 2000 Q2 to 2021 Q4\")\n",
    "    print(f\"Test: {n_test} ({n_test/n_total*100:.1f}%) - 2022 Q1 to 2025 Q3\")\n",
    "    \n",
    "    # Define target variables and their predictors (ALL LAGGED)\n",
    "    target_configs = {\n",
    "        'gdp_growth': {\n",
    "            'target': 'gdp_growth',\n",
    "            'predictors': [\n",
    "                'gdp_growth_lag1', \n",
    "                'gdp_growth_lag4', \n",
    "                'inflation_lag1', \n",
    "                'unemployment_lag1',\n",
    "                'exports_lag1', \n",
    "                'trade_balance_lag1', \n",
    "                'consumption_lag1', \n",
    "                'import_ratio_lag1'\n",
    "            ]\n",
    "        },\n",
    "        'inflation': {\n",
    "            'target': 'inflation',\n",
    "            'predictors': [\n",
    "                'inflation_lag1', \n",
    "                'inflation_lag4', \n",
    "                'inflation_ma4',\n",
    "                'gdp_growth_lag1', \n",
    "                'unemployment_lag1', \n",
    "                'imports_lag1', \n",
    "                'import_ratio_lag1', \n",
    "                'trade_balance_lag1'\n",
    "            ]\n",
    "        },\n",
    "        'unemployment': {\n",
    "            'target': 'unemployment',\n",
    "            'predictors': [\n",
    "                'unemployment_lag1', \n",
    "                'unemployment_lag4',\n",
    "                'gdp_growth_lag1', \n",
    "                'gdp_growth_lag4', \n",
    "                'inflation_lag1',\n",
    "                'emigration_lag1', \n",
    "                'net_migration_lag1', \n",
    "                'exports_lag1', \n",
    "                'consumption_lag1'\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Store all splits\n",
    "    splits = {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PREPARING DATA FOR EACH TARGET VARIABLE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n⚠️  Note: All predictors are LAGGED (t-1, t-4) for realistic forecasting\")\n",
    "    \n",
    "    for target_name, config in target_configs.items():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"TARGET: {target_name.upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        target = config['target']\n",
    "        predictors = config['predictors']\n",
    "        \n",
    "        # Select columns\n",
    "        columns_needed = ['Date', target] + predictors\n",
    "        available_columns = [col for col in columns_needed if col in df.columns]\n",
    "        missing_columns = [col for col in columns_needed if col not in df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"⚠️  Warning: Missing columns: {missing_columns}\")\n",
    "        \n",
    "        # Create target-specific train/test\n",
    "        train_target = train_df[available_columns].copy()\n",
    "        test_target = test_df[available_columns].copy()\n",
    "        \n",
    "        # Drop rows with missing values\n",
    "        train_target_clean = train_target.dropna()\n",
    "        test_target_clean = test_target.dropna()\n",
    "        \n",
    "        # Store in dictionary\n",
    "        splits[target_name] = {\n",
    "            'train': train_target_clean,\n",
    "            'test': test_target_clean,\n",
    "            'target': target,\n",
    "            'predictors': [p for p in predictors if p in available_columns]\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nTarget Variable: {target}\")\n",
    "        print(f\"Predictors ({len(splits[target_name]['predictors'])}): {', '.join(splits[target_name]['predictors'])}\")\n",
    "        \n",
    "        print(f\"\\nTraining Set:\")\n",
    "        print(f\"  Before cleaning: {len(train_target)} observations\")\n",
    "        print(f\"  After cleaning: {len(train_target_clean)} observations\")\n",
    "        print(f\"  Dropped: {len(train_target) - len(train_target_clean)} (due to missing values from lags)\")\n",
    "        \n",
    "        print(f\"\\nTest Set:\")\n",
    "        print(f\"  Before cleaning: {len(test_target)} observations\")\n",
    "        print(f\"  After cleaning: {len(test_target_clean)} observations\")\n",
    "        print(f\"  Dropped: {len(test_target) - len(test_target_clean)} (due to missing values)\")\n",
    "        \n",
    "        # Show statistics\n",
    "        if len(train_target_clean) > 0:\n",
    "            print(f\"\\nTraining Statistics for {target}:\")\n",
    "            print(f\"  Mean: {train_target_clean[target].mean():.2f}\")\n",
    "            print(f\"  Std: {train_target_clean[target].std():.2f}\")\n",
    "            print(f\"  Range: [{train_target_clean[target].min():.2f}, {train_target_clean[target].max():.2f}]\")\n",
    "        \n",
    "        if len(test_target_clean) > 0:\n",
    "            print(f\"\\nTest Statistics for {target}:\")\n",
    "            print(f\"  Mean: {test_target_clean[target].mean():.2f}\")\n",
    "            print(f\"  Std: {test_target_clean[target].std():.2f}\")\n",
    "            print(f\"  Range: [{test_target_clean[target].min():.2f}, {test_target_clean[target].max():.2f}]\")\n",
    "    \n",
    "    return splits\n",
    "\n",
    "\n",
    "def save_splits(splits, output_dir='./'):\n",
    "    \"\"\"\n",
    "    Save train/test splits for each target variable to separate CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    splits : dict\n",
    "        Dictionary containing splits from create_optimal_split()\n",
    "    output_dir : str\n",
    "        Directory to save files (default: current directory)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAVING FILES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    import os\n",
    "    if output_dir != './' and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created directory: {output_dir}\")\n",
    "    \n",
    "    saved_files = []\n",
    "    \n",
    "    for target_name, data in splits.items():\n",
    "        # Save training data\n",
    "        train_filename = f\"{output_dir}train_{target_name}.csv\"\n",
    "        data['train'].to_csv(train_filename, index=False)\n",
    "        saved_files.append(train_filename)\n",
    "        print(f\"✓ Saved: {train_filename} ({len(data['train'])} observations)\")\n",
    "        \n",
    "        # Save test data\n",
    "        test_filename = f\"{output_dir}test_{target_name}.csv\"\n",
    "        data['test'].to_csv(test_filename, index=False)\n",
    "        saved_files.append(test_filename)\n",
    "        print(f\"✓ Saved: {test_filename} ({len(data['test'])} observations)\")\n",
    "    \n",
    "    # Save a summary file\n",
    "    summary_filename = f\"{output_dir}split_summary.txt\"\n",
    "    with open(summary_filename, 'w') as f:\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(\"TRAIN-TEST SPLIT SUMMARY\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        f.write(\"Split Date: 2021-12-31\\n\")\n",
    "        f.write(\"Training Period: 2000 Q2 - 2021 Q4\\n\")\n",
    "        f.write(\"Test Period: 2022 Q1 - 2025 Q3\\n\\n\")\n",
    "        f.write(\"IMPORTANT: All predictors are LAGGED for realistic forecasting\\n\")\n",
    "        f.write(\"  - No contemporaneous values used\\n\")\n",
    "        f.write(\"  - Only historical data (t-1, t-4) available at prediction time\\n\\n\")\n",
    "        \n",
    "        for target_name, data in splits.items():\n",
    "            f.write(f\"\\n{target_name.upper()}:\\n\")\n",
    "            f.write(f\"  Target: {data['target']}\\n\")\n",
    "            f.write(f\"  Predictors ({len(data['predictors'])}): {', '.join(data['predictors'])}\\n\")\n",
    "            f.write(f\"  Training observations: {len(data['train'])}\\n\")\n",
    "            f.write(f\"  Test observations: {len(data['test'])}\\n\")\n",
    "            f.write(f\"  Files: train_{target_name}.csv, test_{target_name}.csv\\n\")\n",
    "    \n",
    "    saved_files.append(summary_filename)\n",
    "    print(f\"\\n✓ Saved: {summary_filename}\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "\n",
    "def load_target_data(target_name, data_dir='./'):\n",
    "    \"\"\"\n",
    "    Convenience function to load train/test data for a specific target.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    target_name : str\n",
    "        Name of target variable ('gdp_growth', 'inflation', 'unemployment')\n",
    "    data_dir : str\n",
    "        Directory containing the CSV files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (X_train, y_train, X_test, y_test, dates_train, dates_test)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    train_df = pd.read_csv(f\"{data_dir}train_{target_name}.csv\")\n",
    "    test_df = pd.read_csv(f\"{data_dir}test_{target_name}.csv\")\n",
    "    \n",
    "    # Convert dates\n",
    "    train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
    "    test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
    "    \n",
    "    # Get target column name\n",
    "    target_configs = {\n",
    "        'gdp_growth': 'gdp_growth',\n",
    "        'inflation': 'inflation',\n",
    "        'unemployment': 'unemployment'\n",
    "    }\n",
    "    target_col = target_configs[target_name]\n",
    "    \n",
    "    # Extract dates\n",
    "    dates_train = train_df['Date']\n",
    "    dates_test = test_df['Date']\n",
    "    \n",
    "    # Extract target and predictors\n",
    "    y_train = train_df[target_col]\n",
    "    y_test = test_df[target_col]\n",
    "    \n",
    "    # Predictors are all columns except Date and target\n",
    "    X_train = train_df.drop(['Date', target_col], axis=1)\n",
    "    X_test = test_df.drop(['Date', target_col], axis=1)\n",
    "    \n",
    "    print(f\"Loaded data for {target_name}:\")\n",
    "    print(f\"  Training: {len(X_train)} observations, {X_train.shape[1]} predictors\")\n",
    "    print(f\"  Test: {len(X_test)} observations, {X_test.shape[1]} predictors\")\n",
    "    print(f\"  Predictors: {list(X_train.columns)}\")\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, dates_train, dates_test\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the feature-engineered data\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv('gen_files/modeling_data_features.csv')\n",
    "    print(f\"Loaded {len(df)} observations\\n\")\n",
    "    \n",
    "    # Create splits for all target variables\n",
    "    splits = create_optimal_split(\n",
    "        df, \n",
    "        date_col='Date',\n",
    "        split_date='2021-12-31'\n",
    "    )\n",
    "    \n",
    "    # Save all splits\n",
    "    saved_files = save_splits(splits, output_dir='./gen_files/')\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DATA SPLITTING COMPLETE!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a699425e",
   "metadata": {},
   "source": [
    "#### Handling Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b71929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data for gdp_growth:\n",
      "  Training: 88 observations, 8 predictors\n",
      "  Test: 15 observations, 8 predictors\n",
      "  Predictors: ['gdp_growth_lag1', 'gdp_growth_lag4', 'inflation_lag1', 'unemployment_lag1', 'exports_lag1', 'trade_balance_lag1', 'consumption_lag1', 'import_ratio_lag1']\n",
      "Loaded data for inflation:\n",
      "  Training: 88 observations, 8 predictors\n",
      "  Test: 15 observations, 8 predictors\n",
      "  Predictors: ['inflation_lag1', 'inflation_lag4', 'inflation_ma4', 'gdp_growth_lag1', 'unemployment_lag1', 'imports_lag1', 'import_ratio_lag1', 'trade_balance_lag1']\n",
      "Loaded data for unemployment:\n",
      "  Training: 88 observations, 9 predictors\n",
      "  Test: 15 observations, 9 predictors\n",
      "  Predictors: ['unemployment_lag1', 'unemployment_lag4', 'gdp_growth_lag1', 'gdp_growth_lag4', 'inflation_lag1', 'emigration_lag1', 'net_migration_lag1', 'exports_lag1', 'consumption_lag1']\n"
     ]
    }
   ],
   "source": [
    "# Load data for modeling:\n",
    "gdp_X_train, gdp_y_train, gdp_X_test, gdp_y_test, gdp_dates_train, gdp_dates_test = load_target_data('gdp_growth', 'gen_files/')\n",
    "inflation_X_train, inflation_y_train, inflation_X_test, inflation_y_test, inflation_dates_train, inflation_dates_test = load_target_data('inflation', 'gen_files/')\n",
    "unemployment_X_train, unemployment_y_train, unemployment_X_test, unemployment_y_test, unemployment_dates_train, unemployment_dates_test = load_target_data('unemployment', 'gen_files/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f867fc39",
   "metadata": {},
   "source": [
    "**Variance Inflation Factor (VIF)**\n",
    "The Variance Inflation Factor measures how much the variance of an estimated regression coefficient increases due to multicollinearity. A VIF value greater than 5 or 10 is considered problematic.\n",
    "\n",
    "[research link](https://www.statology.org/how-to-test-for-multicollinearity-with-statsmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca7f3b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MULTICOLLINEARITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "MULTICOLLINEARITY CHECK: GDP_GROWTH\n",
      "================================================================================\n",
      "\n",
      "1. VIF Analysis (Threshold: VIF > 10)\n",
      "--------------------------------------------------------------------------------\n",
      "           Feature        VIF\n",
      "  consumption_lag1 473.285279\n",
      " import_ratio_lag1 176.017969\n",
      "trade_balance_lag1 126.339523\n",
      "      exports_lag1 120.281059\n",
      " unemployment_lag1  47.805398\n",
      "    inflation_lag1   4.873249\n",
      "   gdp_growth_lag4   2.294437\n",
      "   gdp_growth_lag1   2.102791\n",
      "\n",
      "2. Correlation Analysis (Threshold: |r| > 0.85)\n",
      "--------------------------------------------------------------------------------\n",
      "  exports_lag1              ↔ trade_balance_lag1       : r =  0.963\n",
      "  exports_lag1              ↔ consumption_lag1         : r =  0.986\n",
      "  exports_lag1              ↔ import_ratio_lag1        : r =  0.945\n",
      "  trade_balance_lag1        ↔ consumption_lag1         : r =  0.964\n",
      "  trade_balance_lag1        ↔ import_ratio_lag1        : r =  0.863\n",
      "  consumption_lag1          ↔ import_ratio_lag1        : r =  0.911\n",
      "\n",
      "3. Recommendations\n",
      "--------------------------------------------------------------------------------\n",
      "  Remove 'consumption_lag1' (VIF > 10)\n",
      "  Remove 'import_ratio_lag1' (VIF > 10)\n",
      "  Remove 'trade_balance_lag1' (VIF > 10)\n",
      "  Remove 'exports_lag1' (VIF > 10)\n",
      "  Remove 'unemployment_lag1' (VIF > 10)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Original features: 8\n",
      "Features to remove: 5\n",
      "Final features: 3\n",
      "\n",
      "Removing: trade_balance_lag1, consumption_lag1, exports_lag1, unemployment_lag1, import_ratio_lag1\n",
      "Keeping: gdp_growth_lag1, gdp_growth_lag4, inflation_lag1\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "MULTICOLLINEARITY CHECK: INFLATION\n",
      "================================================================================\n",
      "\n",
      "1. VIF Analysis (Threshold: VIF > 10)\n",
      "--------------------------------------------------------------------------------\n",
      "           Feature       VIF\n",
      "      imports_lag1 79.598802\n",
      "trade_balance_lag1 78.851675\n",
      " import_ratio_lag1 67.712089\n",
      "     inflation_ma4 59.103610\n",
      " unemployment_lag1 40.958888\n",
      "    inflation_lag1 38.740437\n",
      "    inflation_lag4  6.744275\n",
      "   gdp_growth_lag1  1.090578\n",
      "\n",
      "2. Correlation Analysis (Threshold: |r| > 0.85)\n",
      "--------------------------------------------------------------------------------\n",
      "  inflation_lag1            ↔ inflation_ma4            : r =  0.933\n",
      "  imports_lag1              ↔ import_ratio_lag1        : r =  0.952\n",
      "  imports_lag1              ↔ trade_balance_lag1       : r =  0.947\n",
      "  import_ratio_lag1         ↔ trade_balance_lag1       : r =  0.863\n",
      "\n",
      "3. Recommendations\n",
      "--------------------------------------------------------------------------------\n",
      "  Remove 'imports_lag1' (VIF > 10)\n",
      "  Remove 'trade_balance_lag1' (VIF > 10)\n",
      "  Remove 'import_ratio_lag1' (VIF > 10)\n",
      "  Remove 'inflation_ma4' (VIF > 10)\n",
      "  Remove 'unemployment_lag1' (VIF > 10)\n",
      "  Remove 'inflation_lag1' (VIF > 10)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Original features: 8\n",
      "Features to remove: 6\n",
      "Final features: 2\n",
      "\n",
      "Removing: trade_balance_lag1, imports_lag1, inflation_ma4, inflation_lag1, unemployment_lag1, import_ratio_lag1\n",
      "Keeping: inflation_lag4, gdp_growth_lag1\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "MULTICOLLINEARITY CHECK: UNEMPLOYMENT\n",
      "================================================================================\n",
      "\n",
      "1. VIF Analysis (Threshold: VIF > 10)\n",
      "--------------------------------------------------------------------------------\n",
      "           Feature        VIF\n",
      "  consumption_lag1 903.758685\n",
      "   emigration_lag1 357.659401\n",
      " unemployment_lag4 106.139339\n",
      " unemployment_lag1  93.379179\n",
      "net_migration_lag1  71.209204\n",
      "      exports_lag1  59.757906\n",
      "    inflation_lag1   4.363796\n",
      "   gdp_growth_lag1   2.395893\n",
      "   gdp_growth_lag4   2.359652\n",
      "\n",
      "2. Correlation Analysis (Threshold: |r| > 0.85)\n",
      "--------------------------------------------------------------------------------\n",
      "  unemployment_lag1         ↔ unemployment_lag4        : r =  0.898\n",
      "  unemployment_lag4         ↔ consumption_lag1         : r =  0.851\n",
      "  emigration_lag1           ↔ exports_lag1             : r =  0.964\n",
      "  emigration_lag1           ↔ consumption_lag1         : r =  0.939\n",
      "  exports_lag1              ↔ consumption_lag1         : r =  0.986\n",
      "\n",
      "3. Recommendations\n",
      "--------------------------------------------------------------------------------\n",
      "  Remove 'consumption_lag1' (VIF > 10)\n",
      "  Remove 'emigration_lag1' (VIF > 10)\n",
      "  Remove 'unemployment_lag4' (VIF > 10)\n",
      "  Remove 'unemployment_lag1' (VIF > 10)\n",
      "  Remove 'net_migration_lag1' (VIF > 10)\n",
      "  Remove 'exports_lag1' (VIF > 10)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Original features: 9\n",
      "Features to remove: 6\n",
      "Final features: 3\n",
      "\n",
      "Removing: consumption_lag1, exports_lag1, emigration_lag1, unemployment_lag4, unemployment_lag1, net_migration_lag1\n",
      "Keeping: gdp_growth_lag1, gdp_growth_lag4, inflation_lag1\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "      Target  Original  Final  Removed\n",
      "  gdp_growth         8      3        5\n",
      "   inflation         8      2        6\n",
      "unemployment         9      3        6\n",
      "\n",
      "================================================================================\n",
      "CLEANED DATA READY!\n",
      "================================================================================\n",
      "\n",
      "Access cleaned data:\n",
      "  cleaned_data['gdp_growth']['X_train']\n",
      "  cleaned_data['gdp_growth']['X_test']\n",
      "  cleaned_data['inflation']['X_train']\n",
      "  cleaned_data['inflation']['X_test']\n",
      "  cleaned_data['unemployment']['X_train']\n",
      "  cleaned_data['unemployment']['X_test']\n",
      "\n",
      "✓ Ready for model training!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def check_multicollinearity(X_train, y_train, target_name):\n",
    "    \"\"\"\n",
    "    Simple multicollinearity check using VIF and correlation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas.DataFrame\n",
    "        Training predictors\n",
    "    y_train : pandas.Series\n",
    "        Training target\n",
    "    target_name : str\n",
    "        Name of target variable\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Features to remove and keep\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"MULTICOLLINEARITY CHECK: {target_name.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Calculate VIF\n",
    "    print(\"\\n1. VIF Analysis (Threshold: VIF > 10)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    vif_data = []\n",
    "    for i, col in enumerate(X_train.columns):\n",
    "        vif = variance_inflation_factor(X_train.values, i)\n",
    "        vif_data.append({'Feature': col, 'VIF': vif})\n",
    "    \n",
    "    vif_df = pd.DataFrame(vif_data).sort_values('VIF', ascending=False)\n",
    "    print(vif_df.to_string(index=False))\n",
    "    \n",
    "    # Flag high VIF\n",
    "    high_vif = vif_df[vif_df['VIF'] > 10]['Feature'].tolist()\n",
    "    \n",
    "    # 2. Check correlations\n",
    "    print(\"\\n2. Correlation Analysis (Threshold: |r| > 0.85)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    corr_matrix = X_train.corr()\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_val = corr_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.85:\n",
    "                feat1 = corr_matrix.columns[i]\n",
    "                feat2 = corr_matrix.columns[j]\n",
    "                high_corr_pairs.append((feat1, feat2, corr_val))\n",
    "                print(f\"  {feat1:25s} ↔ {feat2:25s}: r = {corr_val:6.3f}\")\n",
    "    \n",
    "    if not high_corr_pairs:\n",
    "        print(\"  ✓ No high correlations found\")\n",
    "    \n",
    "    # 3. Determine features to remove\n",
    "    print(\"\\n3. Recommendations\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    features_to_remove = set()\n",
    "    \n",
    "    # Remove features with high VIF\n",
    "    for feat in high_vif:\n",
    "        features_to_remove.add(feat)\n",
    "        print(f\"  Remove '{feat}' (VIF > 10)\")\n",
    "    \n",
    "    # Remove one from each highly correlated pair\n",
    "    # Keep the one with stronger target correlation\n",
    "    target_corr = X_train.corrwith(y_train).abs()\n",
    "    \n",
    "    for feat1, feat2, corr_val in high_corr_pairs:\n",
    "        if feat1 not in features_to_remove and feat2 not in features_to_remove:\n",
    "            # Remove the one with weaker target correlation\n",
    "            if target_corr[feat1] > target_corr[feat2]:\n",
    "                features_to_remove.add(feat2)\n",
    "                print(f\"  Remove '{feat2}' (correlated with '{feat1}', r={corr_val:.3f})\")\n",
    "            else:\n",
    "                features_to_remove.add(feat1)\n",
    "                print(f\"  Remove '{feat1}' (correlated with '{feat2}', r={corr_val:.3f})\")\n",
    "    \n",
    "    features_to_remove = list(features_to_remove)\n",
    "    features_to_keep = [f for f in X_train.columns if f not in features_to_remove]\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Original features: {len(X_train.columns)}\")\n",
    "    print(f\"Features to remove: {len(features_to_remove)}\")\n",
    "    print(f\"Final features: {len(features_to_keep)}\")\n",
    "    \n",
    "    if features_to_remove:\n",
    "        print(f\"\\nRemoving: {', '.join(features_to_remove)}\")\n",
    "        print(f\"Keeping: {', '.join(features_to_keep)}\")\n",
    "    else:\n",
    "        print(\"\\n✓ No multicollinearity issues - keeping all features\")\n",
    "    \n",
    "    return {\n",
    "        'features_to_remove': features_to_remove,\n",
    "        'features_to_keep': features_to_keep,\n",
    "        'vif': vif_df\n",
    "    }\n",
    "\n",
    "\n",
    "def remove_collinear_features(X_train, X_test, features_to_remove):\n",
    "    \"\"\"\n",
    "    Remove features from train and test sets.\n",
    "    \"\"\"\n",
    "    if not features_to_remove:\n",
    "        return X_train.copy(), X_test.copy()\n",
    "    \n",
    "    X_train_clean = X_train.drop(columns=features_to_remove)\n",
    "    X_test_clean = X_test.drop(columns=features_to_remove)\n",
    "    \n",
    "    return X_train_clean, X_test_clean\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MULTICOLLINEARITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Store cleaned data\n",
    "cleaned_data = {}\n",
    "\n",
    "# 1. GDP GROWTH\n",
    "print(\"\\n\")\n",
    "gdp_results = check_multicollinearity(gdp_X_train, gdp_y_train, 'gdp_growth')\n",
    "gdp_X_train_clean, gdp_X_test_clean = remove_collinear_features(\n",
    "    gdp_X_train, gdp_X_test, gdp_results['features_to_remove']\n",
    ")\n",
    "cleaned_data['gdp_growth'] = {\n",
    "    'X_train': gdp_X_train_clean,\n",
    "    'y_train': gdp_y_train,\n",
    "    'X_test': gdp_X_test_clean,\n",
    "    'y_test': gdp_y_test\n",
    "}\n",
    "\n",
    "# 2. INFLATION\n",
    "print(\"\\n\\n\")\n",
    "inflation_results = check_multicollinearity(inflation_X_train, inflation_y_train, 'inflation')\n",
    "inflation_X_train_clean, inflation_X_test_clean = remove_collinear_features(\n",
    "    inflation_X_train, inflation_X_test, inflation_results['features_to_remove']\n",
    ")\n",
    "cleaned_data['inflation'] = {\n",
    "    'X_train': inflation_X_train_clean,\n",
    "    'y_train': inflation_y_train,\n",
    "    'X_test': inflation_X_test_clean,\n",
    "    'y_test': inflation_y_test\n",
    "}\n",
    "\n",
    "# 3. UNEMPLOYMENT\n",
    "print(\"\\n\\n\")\n",
    "unemployment_results = check_multicollinearity(unemployment_X_train, unemployment_y_train, 'unemployment')\n",
    "unemployment_X_train_clean, unemployment_X_test_clean = remove_collinear_features(\n",
    "    unemployment_X_train, unemployment_X_test, unemployment_results['features_to_remove']\n",
    ")\n",
    "cleaned_data['unemployment'] = {\n",
    "    'X_train': unemployment_X_train_clean,\n",
    "    'y_train': unemployment_y_train,\n",
    "    'X_test': unemployment_X_test_clean,\n",
    "    'y_test': unemployment_y_test\n",
    "}\n",
    "\n",
    "# FINAL SUMMARY\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_data = []\n",
    "for target in ['gdp_growth', 'inflation', 'unemployment']:\n",
    "    data = cleaned_data[target]\n",
    "    summary_data.append({\n",
    "        'Target': target,\n",
    "        'Original': gdp_X_train.shape[1] if target == 'gdp_growth' else \n",
    "                   inflation_X_train.shape[1] if target == 'inflation' else \n",
    "                   unemployment_X_train.shape[1],\n",
    "        'Final': data['X_train'].shape[1],\n",
    "        'Removed': (gdp_X_train.shape[1] if target == 'gdp_growth' else \n",
    "                   inflation_X_train.shape[1] if target == 'inflation' else \n",
    "                   unemployment_X_train.shape[1]) - data['X_train'].shape[1]\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLEANED DATA READY!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nAccess cleaned data:\")\n",
    "print(\"  cleaned_data['gdp_growth']['X_train']\")\n",
    "print(\"  cleaned_data['gdp_growth']['X_test']\")\n",
    "print(\"  cleaned_data['inflation']['X_train']\")\n",
    "print(\"  cleaned_data['inflation']['X_test']\")\n",
    "print(\"  cleaned_data['unemployment']['X_train']\")\n",
    "print(\"  cleaned_data['unemployment']['X_test']\")\n",
    "\n",
    "print(\"\\n✓ Ready for model training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7843a20",
   "metadata": {},
   "source": [
    "### Model Training & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df00b54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 2: MODEL TRAINING & EVALUATION\n",
      "================================================================================\n",
      "\n",
      "\n",
      "################################################################################\n",
      "# PROCESSING TARGET: GDP_GROWTH\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "TRAINING ALL MODELS FOR: GDP_GROWTH\n",
      "================================================================================\n",
      "\n",
      "Training set: 88 observations\n",
      "Test set: 15 observations\n",
      "Predictors: 3\n",
      "\n",
      "================================================================================\n",
      "MODEL 1: MULTIPLE LINEAR REGRESSION - GDP_GROWTH\n",
      "================================================================================\n",
      "\n",
      "Training Performance:\n",
      "  R² = 0.7868\n",
      "  RMSE = 2.8890\n",
      "  MAE = 2.2480\n",
      "  MAPE = 138.1578%\n",
      "\n",
      "Test Performance:\n",
      "  R² = 0.7964\n",
      "  RMSE = 2.7884\n",
      "  MAE = 2.3450\n",
      "  MAPE = nan%\n",
      "\n",
      "Model Coefficients:\n",
      "        Feature  Coefficient\n",
      "gdp_growth_lag4     0.837905\n",
      " inflation_lag1    -0.430361\n",
      "gdp_growth_lag1    -0.101318\n",
      "\n",
      "================================================================================\n",
      "MODEL 2: POLYNOMIAL REGRESSION (degree=2) - GDP_GROWTH\n",
      "================================================================================\n",
      "\n",
      "Original features: 3\n",
      "Polynomial features: 9\n",
      "\n",
      "Training Performance:\n",
      "  R² = 0.8032\n",
      "  RMSE = 2.7759\n",
      "  MAE = 2.0919\n",
      "  MAPE = 132.3339%\n",
      "\n",
      "Test Performance:\n",
      "  R² = 0.7227\n",
      "  RMSE = 3.2544\n",
      "  MAE = 2.8510\n",
      "  MAPE = nan%\n",
      "\n",
      "================================================================================\n",
      "MODEL 3: HP-FILTERED REGRESSION (λ=1600) - GDP_GROWTH\n",
      "================================================================================\n",
      "\n",
      "Applying HP filter to target variable...\n",
      "Applying HP filter to predictors...\n",
      "\n",
      "Training Performance:\n",
      "  R² = 0.7995\n",
      "  RMSE = 2.8019\n",
      "  MAE = 2.1439\n",
      "  MAPE = 128.7405%\n",
      "\n",
      "Test Performance:\n",
      "  R² = 0.7777\n",
      "  RMSE = 2.9140\n",
      "  MAE = 2.3373\n",
      "  MAPE = nan%\n",
      "\n",
      "================================================================================\n",
      "MODEL 4: ARIMAX(1, 0, 1)x(0, 0, 1, 4) - GDP_GROWTH\n",
      "================================================================================\n",
      "\n",
      "Fitting ARIMAX(1, 0, 1)x(0, 0, 1, 4)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Documents\\UTM-Master\\EDA\\ExploratoryDataAnalysis\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\filters\\hp_filter.py:100: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
      "  trend = spsolve(I+lamb*K.T.dot(K), x, use_umfpack=use_umfpack)\n",
      "c:\\Users\\admin\\Documents\\UTM-Master\\EDA\\ExploratoryDataAnalysis\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\filters\\hp_filter.py:100: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
      "  trend = spsolve(I+lamb*K.T.dot(K), x, use_umfpack=use_umfpack)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model converged successfully\n",
      "\n",
      "Training Performance:\n",
      "  R² = 0.8401\n",
      "  RMSE = 2.5023\n",
      "  MAE = 1.9964\n",
      "  MAPE = 161.4916%\n",
      "\n",
      "Test Performance:\n",
      "  R² = 0.8105\n",
      "  RMSE = 2.6902\n",
      "  MAE = 2.1693\n",
      "  MAPE = nan%\n",
      "\n",
      "Model AIC: 392.35\n",
      "Model BIC: 409.19\n",
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON: GDP_GROWTH\n",
      "================================================================================\n",
      "\n",
      "     Model  Train R²  Test R²  Train RMSE  Test RMSE  Train MAE  Test MAE  Test MAPE\n",
      "    Linear  0.786807 0.796423    2.888955   2.788414   2.247993  2.345032        NaN\n",
      "Polynomial  0.803160 0.722688    2.775946   3.254447   2.091897  2.850993        NaN\n",
      " Hp Filter  0.799456 0.777680    2.801942   2.913952   2.143945  2.337261        NaN\n",
      "    Arimax  0.840051 0.810504    2.502332   2.690249   1.996409  2.169337        NaN\n",
      "\n",
      "================================================================================\n",
      "BEST MODELS:\n",
      "================================================================================\n",
      "  Highest Test R²: Arimax (R² = 0.8105)\n",
      "  Lowest Test RMSE: Arimax (RMSE = 2.6902)\n",
      "\n",
      "\n",
      "################################################################################\n",
      "# PROCESSING TARGET: INFLATION\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "TRAINING ALL MODELS FOR: INFLATION\n",
      "================================================================================\n",
      "\n",
      "Training set: 88 observations\n",
      "Test set: 15 observations\n",
      "Predictors: 2\n",
      "\n",
      "================================================================================\n",
      "MODEL 1: MULTIPLE LINEAR REGRESSION - INFLATION\n",
      "================================================================================\n",
      "\n",
      "Training Performance:\n",
      "  R² = 0.0079\n",
      "  RMSE = 1.4583\n",
      "  MAE = 1.1703\n",
      "  MAPE = 253.7142%\n",
      "\n",
      "Test Performance:\n",
      "  R² = -0.4264\n",
      "  RMSE = 3.1675\n",
      "  MAE = 2.0403\n",
      "  MAPE = 38.8343%\n",
      "\n",
      "Model Coefficients:\n",
      "        Feature  Coefficient\n",
      " inflation_lag4     0.045167\n",
      "gdp_growth_lag1     0.018380\n",
      "\n",
      "================================================================================\n",
      "MODEL 2: POLYNOMIAL REGRESSION (degree=2) - INFLATION\n",
      "================================================================================\n",
      "\n",
      "Original features: 2\n",
      "Polynomial features: 5\n",
      "\n",
      "Training Performance:\n",
      "  R² = 0.0752\n",
      "  RMSE = 1.4079\n",
      "  MAE = 1.1481\n",
      "  MAPE = 265.0843%\n",
      "\n",
      "Test Performance:\n",
      "  R² = -1.1256\n",
      "  RMSE = 3.8667\n",
      "  MAE = 2.8584\n",
      "  MAPE = 68.6932%\n",
      "\n",
      "================================================================================\n",
      "MODEL 3: HP-FILTERED REGRESSION (λ=1600) - INFLATION\n",
      "================================================================================\n",
      "\n",
      "Applying HP filter to target variable...\n",
      "Applying HP filter to predictors...\n",
      "\n",
      "Training Performance:\n",
      "  R² = 0.4132\n",
      "  RMSE = 1.1215\n",
      "  MAE = 0.9236\n",
      "  MAPE = 166.1206%\n",
      "\n",
      "Test Performance:\n",
      "  R² = 0.7622\n",
      "  RMSE = 1.2934\n",
      "  MAE = 1.0901\n",
      "  MAPE = 34.3485%\n",
      "\n",
      "================================================================================\n",
      "MODEL 4: ARIMAX(1, 0, 1)x(0, 0, 1, 4) - INFLATION\n",
      "================================================================================\n",
      "\n",
      "Fitting ARIMAX(1, 0, 1)x(0, 0, 1, 4)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Documents\\UTM-Master\\EDA\\ExploratoryDataAnalysis\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\filters\\hp_filter.py:100: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
      "  trend = spsolve(I+lamb*K.T.dot(K), x, use_umfpack=use_umfpack)\n",
      "c:\\Users\\admin\\Documents\\UTM-Master\\EDA\\ExploratoryDataAnalysis\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\filters\\hp_filter.py:100: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
      "  trend = spsolve(I+lamb*K.T.dot(K), x, use_umfpack=use_umfpack)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model converged successfully\n",
      "\n",
      "Training Performance:\n",
      "  R² = 0.7409\n",
      "  RMSE = 0.7452\n",
      "  MAE = 0.5447\n",
      "  MAPE = 97.8431%\n",
      "\n",
      "Test Performance:\n",
      "  R² = -0.2585\n",
      "  RMSE = 2.9753\n",
      "  MAE = 2.3251\n",
      "  MAPE = nan%\n",
      "\n",
      "Model AIC: 180.14\n",
      "Model BIC: 194.58\n",
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON: INFLATION\n",
      "================================================================================\n",
      "\n",
      "     Model  Train R²   Test R²  Train RMSE  Test RMSE  Train MAE  Test MAE  Test MAPE\n",
      "    Linear  0.007900 -0.426408    1.458311   3.167524   1.170300  2.040278  38.834326\n",
      "Polynomial  0.075248 -1.125626    1.407943   3.866708   1.148114  2.858428  68.693154\n",
      " Hp Filter  0.413202  0.762184    1.121545   1.293356   0.923588  1.090137  34.348492\n",
      "    Arimax  0.740927 -0.258511    0.745219   2.975271   0.544725  2.325092        NaN\n",
      "\n",
      "================================================================================\n",
      "BEST MODELS:\n",
      "================================================================================\n",
      "  Highest Test R²: Hp Filter (R² = 0.7622)\n",
      "  Lowest Test RMSE: Hp Filter (RMSE = 1.2934)\n",
      "\n",
      "\n",
      "################################################################################\n",
      "# PROCESSING TARGET: UNEMPLOYMENT\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "TRAINING ALL MODELS FOR: UNEMPLOYMENT\n",
      "================================================================================\n",
      "\n",
      "Training set: 88 observations\n",
      "Test set: 15 observations\n",
      "Predictors: 3\n",
      "\n",
      "================================================================================\n",
      "MODEL 1: MULTIPLE LINEAR REGRESSION - UNEMPLOYMENT\n",
      "================================================================================\n",
      "\n",
      "Training Performance:\n",
      "  R² = 0.2261\n",
      "  RMSE = 1.1431\n",
      "  MAE = 0.8871\n",
      "  MAPE = 25.8791%\n",
      "\n",
      "Test Performance:\n",
      "  R² = -3.5827\n",
      "  RMSE = 1.7889\n",
      "  MAE = 1.6490\n",
      "  MAPE = 30.4609%\n",
      "\n",
      "Model Coefficients:\n",
      "        Feature  Coefficient\n",
      " inflation_lag1    -0.442386\n",
      "gdp_growth_lag4     0.017079\n",
      "gdp_growth_lag1     0.016884\n",
      "\n",
      "================================================================================\n",
      "MODEL 2: POLYNOMIAL REGRESSION (degree=2) - UNEMPLOYMENT\n",
      "================================================================================\n",
      "\n",
      "Original features: 3\n",
      "Polynomial features: 9\n",
      "\n",
      "Training Performance:\n",
      "  R² = 0.3390\n",
      "  RMSE = 1.0564\n",
      "  MAE = 0.8137\n",
      "  MAPE = 23.5423%\n",
      "\n",
      "Test Performance:\n",
      "  R² = -4.3710\n",
      "  RMSE = 1.9367\n",
      "  MAE = 1.7851\n",
      "  MAPE = 32.9014%\n",
      "\n",
      "================================================================================\n",
      "MODEL 3: HP-FILTERED REGRESSION (λ=1600) - UNEMPLOYMENT\n",
      "================================================================================\n",
      "\n",
      "Applying HP filter to target variable...\n",
      "Applying HP filter to predictors...\n",
      "\n",
      "Training Performance:\n",
      "  R² = 0.8680\n",
      "  RMSE = 0.4720\n",
      "  MAE = 0.3672\n",
      "  MAPE = 8.8342%\n",
      "\n",
      "Test Performance:\n",
      "  R² = 0.8817\n",
      "  RMSE = 0.2874\n",
      "  MAE = 0.2335\n",
      "  MAPE = 4.3053%\n",
      "\n",
      "================================================================================\n",
      "MODEL 4: ARIMAX(1, 0, 1)x(0, 0, 1, 4) - UNEMPLOYMENT\n",
      "================================================================================\n",
      "\n",
      "Fitting ARIMAX(1, 0, 1)x(0, 0, 1, 4)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Documents\\UTM-Master\\EDA\\ExploratoryDataAnalysis\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\filters\\hp_filter.py:100: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
      "  trend = spsolve(I+lamb*K.T.dot(K), x, use_umfpack=use_umfpack)\n",
      "c:\\Users\\admin\\Documents\\UTM-Master\\EDA\\ExploratoryDataAnalysis\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\filters\\hp_filter.py:100: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
      "  trend = spsolve(I+lamb*K.T.dot(K), x, use_umfpack=use_umfpack)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model converged successfully\n",
      "\n",
      "Training Performance:\n",
      "  R² = 0.9069\n",
      "  RMSE = 0.3965\n",
      "  MAE = 0.2644\n",
      "  MAPE = 6.0989%\n",
      "\n",
      "Test Performance:\n",
      "  R² = -3.5901\n",
      "  RMSE = 1.7903\n",
      "  MAE = 1.4851\n",
      "  MAPE = nan%\n",
      "\n",
      "Model AIC: 60.32\n",
      "Model BIC: 77.17\n",
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON: UNEMPLOYMENT\n",
      "================================================================================\n",
      "\n",
      "     Model  Train R²   Test R²  Train RMSE  Test RMSE  Train MAE  Test MAE  Test MAPE\n",
      "    Linear  0.226080 -3.582674    1.143071   1.788894   0.887105  1.648980  30.460888\n",
      "Polynomial  0.339020 -4.370996    1.056378   1.936657   0.813722  1.785111  32.901380\n",
      " Hp Filter  0.868023  0.881739    0.472035   0.287373   0.367153  0.233497   4.305328\n",
      "    Arimax  0.906862 -3.590110    0.396541   1.790345   0.264384  1.485123        NaN\n",
      "\n",
      "================================================================================\n",
      "BEST MODELS:\n",
      "================================================================================\n",
      "  Highest Test R²: Hp Filter (R² = 0.8817)\n",
      "  Lowest Test RMSE: Hp Filter (RMSE = 0.2874)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from statsmodels.tsa.filters.hp_filter import hpfilter\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive evaluation metrics.\n",
    "    \"\"\"\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    # MAPE (handle zero values)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100 if (y_true != 0).all() else np.nan\n",
    "    \n",
    "    # Adjusted R² (if we know n and p)\n",
    "    n = len(y_true)\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'R²': r2,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_predictions(y_train, y_test, y_pred_train, y_pred_test, dates_train, dates_test, \n",
    "                     target_name, model_name):\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted values for train and test sets.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Training set\n",
    "    ax1.plot(dates_train, y_train, label='Actual', color='black', linewidth=2)\n",
    "    ax1.plot(dates_train, y_pred_train, label='Predicted', color='blue', linewidth=1.5, alpha=0.7)\n",
    "    ax1.fill_between(dates_train, y_train, y_pred_train, alpha=0.3, color='blue')\n",
    "    ax1.set_title(f'{model_name} - Training Set: {target_name.upper()}', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel(target_name)\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # Test set\n",
    "    ax2.plot(dates_test, y_test, label='Actual', color='black', linewidth=2)\n",
    "    ax2.plot(dates_test, y_pred_test, label='Predicted', color='red', linewidth=1.5, alpha=0.7)\n",
    "    ax2.fill_between(dates_test, y_test, y_pred_test, alpha=0.3, color='red')\n",
    "    ax2.set_title(f'{model_name} - Test Set: {target_name.upper()}', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Date')\n",
    "    ax2.set_ylabel(target_name)\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 1: MULTIPLE LINEAR REGRESSION\n",
    "# ============================================================================\n",
    "\n",
    "def train_linear_regression(X_train, y_train, X_test, y_test, target_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate Multiple Linear Regression model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL 1: MULTIPLE LINEAR REGRESSION - {target_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Train model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    train_metrics = calculate_metrics(y_train, y_pred_train, \"Linear Regression (Train)\")\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test, \"Linear Regression (Test)\")\n",
    "    \n",
    "    print(\"\\nTraining Performance:\")\n",
    "    print(f\"  R² = {train_metrics['R²']:.4f}\")\n",
    "    print(f\"  RMSE = {train_metrics['RMSE']:.4f}\")\n",
    "    print(f\"  MAE = {train_metrics['MAE']:.4f}\")\n",
    "    print(f\"  MAPE = {train_metrics['MAPE']:.4f}%\")\n",
    "    \n",
    "    print(\"\\nTest Performance:\")\n",
    "    print(f\"  R² = {test_metrics['R²']:.4f}\")\n",
    "    print(f\"  RMSE = {test_metrics['RMSE']:.4f}\")\n",
    "    print(f\"  MAE = {test_metrics['MAE']:.4f}\")\n",
    "    print(f\"  MAPE = {test_metrics['MAPE']:.4f}%\")\n",
    "    \n",
    "    # Coefficients\n",
    "    print(\"\\nModel Coefficients:\")\n",
    "    coef_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Coefficient': model.coef_\n",
    "    }).sort_values('Coefficient', key=abs, ascending=False)\n",
    "    print(coef_df.to_string(index=False))\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'train_metrics': train_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'y_pred_train': y_pred_train,\n",
    "        'y_pred_test': y_pred_test,\n",
    "        'coefficients': coef_df\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 2: POLYNOMIAL REGRESSION\n",
    "# ============================================================================\n",
    "\n",
    "def train_polynomial_regression(X_train, y_train, X_test, y_test, target_name, degree=2):\n",
    "    \"\"\"\n",
    "    Train and evaluate Polynomial Regression model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL 2: POLYNOMIAL REGRESSION (degree={degree}) - {target_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    \n",
    "    print(f\"\\nOriginal features: {X_train.shape[1]}\")\n",
    "    print(f\"Polynomial features: {X_train_poly.shape[1]}\")\n",
    "    \n",
    "    # Standardize (important for polynomial features)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_poly_scaled = scaler.fit_transform(X_train_poly)\n",
    "    X_test_poly_scaled = scaler.transform(X_test_poly)\n",
    "    \n",
    "    # Train model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train_poly_scaled)\n",
    "    y_pred_test = model.predict(X_test_poly_scaled)\n",
    "    \n",
    "    # Metrics\n",
    "    train_metrics = calculate_metrics(y_train, y_pred_train, f\"Polynomial (degree={degree}, Train)\")\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test, f\"Polynomial (degree={degree}, Test)\")\n",
    "    \n",
    "    print(\"\\nTraining Performance:\")\n",
    "    print(f\"  R² = {train_metrics['R²']:.4f}\")\n",
    "    print(f\"  RMSE = {train_metrics['RMSE']:.4f}\")\n",
    "    print(f\"  MAE = {train_metrics['MAE']:.4f}\")\n",
    "    print(f\"  MAPE = {train_metrics['MAPE']:.4f}%\")\n",
    "    \n",
    "    print(\"\\nTest Performance:\")\n",
    "    print(f\"  R² = {test_metrics['R²']:.4f}\")\n",
    "    print(f\"  RMSE = {test_metrics['RMSE']:.4f}\")\n",
    "    print(f\"  MAE = {test_metrics['MAE']:.4f}\")\n",
    "    print(f\"  MAPE = {test_metrics['MAPE']:.4f}%\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'poly': poly,\n",
    "        'scaler': scaler,\n",
    "        'train_metrics': train_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'y_pred_train': y_pred_train,\n",
    "        'y_pred_test': y_pred_test\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 3: HP-FILTERED REGRESSION\n",
    "# ============================================================================\n",
    "\n",
    "def train_hp_filter_regression(X_train, y_train, X_test, y_test, target_name, lamb=1600):\n",
    "    \"\"\"\n",
    "    Train and evaluate HP-Filtered Regression model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL 3: HP-FILTERED REGRESSION (λ={lamb}) - {target_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Apply HP filter to target\n",
    "    print(\"\\nApplying HP filter to target variable...\")\n",
    "    y_train_cycle, y_train_trend = hpfilter(y_train, lamb=lamb)\n",
    "    y_test_cycle, y_test_trend = hpfilter(y_test, lamb=lamb)\n",
    "    \n",
    "    # Apply HP filter to predictors\n",
    "    print(\"Applying HP filter to predictors...\")\n",
    "    X_train_cycle = pd.DataFrame(index=X_train.index, columns=X_train.columns)\n",
    "    X_test_cycle = pd.DataFrame(index=X_test.index, columns=X_test.columns)\n",
    "    \n",
    "    for col in X_train.columns:\n",
    "        try:\n",
    "            cycle_train, _ = hpfilter(X_train[col], lamb=lamb)\n",
    "            cycle_test, _ = hpfilter(X_test[col], lamb=lamb)\n",
    "            X_train_cycle[col] = cycle_train\n",
    "            X_test_cycle[col] = cycle_test\n",
    "        except:\n",
    "            print(f\"  ⚠️  Could not filter {col}, using original values\")\n",
    "            X_train_cycle[col] = X_train[col]\n",
    "            X_test_cycle[col] = X_test[col]\n",
    "    \n",
    "    # Train model on cyclical components\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_cycle, y_train_cycle)\n",
    "    \n",
    "    # Predictions (cyclical components)\n",
    "    y_pred_train_cycle = model.predict(X_train_cycle)\n",
    "    y_pred_test_cycle = model.predict(X_test_cycle)\n",
    "    \n",
    "    # Add trend back to get final predictions\n",
    "    y_pred_train = y_pred_train_cycle + y_train_trend\n",
    "    y_pred_test = y_pred_test_cycle + y_test_trend\n",
    "    \n",
    "    # Metrics\n",
    "    train_metrics = calculate_metrics(y_train, y_pred_train, \"HP-Filtered (Train)\")\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test, \"HP-Filtered (Test)\")\n",
    "    \n",
    "    print(\"\\nTraining Performance:\")\n",
    "    print(f\"  R² = {train_metrics['R²']:.4f}\")\n",
    "    print(f\"  RMSE = {train_metrics['RMSE']:.4f}\")\n",
    "    print(f\"  MAE = {train_metrics['MAE']:.4f}\")\n",
    "    print(f\"  MAPE = {train_metrics['MAPE']:.4f}%\")\n",
    "    \n",
    "    print(\"\\nTest Performance:\")\n",
    "    print(f\"  R² = {test_metrics['R²']:.4f}\")\n",
    "    print(f\"  RMSE = {test_metrics['RMSE']:.4f}\")\n",
    "    print(f\"  MAE = {test_metrics['MAE']:.4f}\")\n",
    "    print(f\"  MAPE = {test_metrics['MAPE']:.4f}%\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'train_metrics': train_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'y_pred_train': y_pred_train,\n",
    "        'y_pred_test': y_pred_test,\n",
    "        'y_train_cycle': y_train_cycle,\n",
    "        'y_test_cycle': y_test_cycle,\n",
    "        'y_train_trend': y_train_trend,\n",
    "        'y_test_trend': y_test_trend\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 4: ARIMAX\n",
    "# ============================================================================\n",
    "\n",
    "def train_arimax(X_train, y_train, X_test, y_test, target_name, order=(1,0,1), seasonal_order=(0,0,1,4)):\n",
    "    \"\"\"\n",
    "    Train and evaluate ARIMAX model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL 4: ARIMAX{order}x{seasonal_order} - {target_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Train ARIMAX model\n",
    "        print(f\"\\nFitting ARIMAX{order}x{seasonal_order}...\")\n",
    "        model = SARIMAX(\n",
    "            y_train,\n",
    "            exog=X_train,\n",
    "            order=order,\n",
    "            seasonal_order=seasonal_order,\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False\n",
    "        )\n",
    "        \n",
    "        results = model.fit(disp=False, maxiter=200)\n",
    "        \n",
    "        print(\"✓ Model converged successfully\")\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_train = results.fittedvalues\n",
    "        y_pred_test = results.forecast(steps=len(y_test), exog=X_test)\n",
    "        \n",
    "        # Metrics\n",
    "        train_metrics = calculate_metrics(y_train, y_pred_train, f\"ARIMAX{order} (Train)\")\n",
    "        test_metrics = calculate_metrics(y_test, y_pred_test, f\"ARIMAX{order} (Test)\")\n",
    "        \n",
    "        print(\"\\nTraining Performance:\")\n",
    "        print(f\"  R² = {train_metrics['R²']:.4f}\")\n",
    "        print(f\"  RMSE = {train_metrics['RMSE']:.4f}\")\n",
    "        print(f\"  MAE = {train_metrics['MAE']:.4f}\")\n",
    "        print(f\"  MAPE = {train_metrics['MAPE']:.4f}%\")\n",
    "        \n",
    "        print(\"\\nTest Performance:\")\n",
    "        print(f\"  R² = {test_metrics['R²']:.4f}\")\n",
    "        print(f\"  RMSE = {test_metrics['RMSE']:.4f}\")\n",
    "        print(f\"  MAE = {test_metrics['MAE']:.4f}\")\n",
    "        print(f\"  MAPE = {test_metrics['MAPE']:.4f}%\")\n",
    "        \n",
    "        print(f\"\\nModel AIC: {results.aic:.2f}\")\n",
    "        print(f\"Model BIC: {results.bic:.2f}\")\n",
    "        \n",
    "        return {\n",
    "            'model': results,\n",
    "            'train_metrics': train_metrics,\n",
    "            'test_metrics': test_metrics,\n",
    "            'y_pred_train': y_pred_train,\n",
    "            'y_pred_test': y_pred_test,\n",
    "            'aic': results.aic,\n",
    "            'bic': results.bic\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ARIMAX model failed to converge: {e}\")\n",
    "        print(\"Returning None - will be excluded from comparison\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def train_all_models(X_train, y_train, X_test, y_test, dates_train, dates_test, target_name):\n",
    "    \"\"\"\n",
    "    Train all 4 models for a given target variable.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"TRAINING ALL MODELS FOR: {target_name.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTraining set: {len(X_train)} observations\")\n",
    "    print(f\"Test set: {len(X_test)} observations\")\n",
    "    print(f\"Predictors: {X_train.shape[1]}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Model 1: Linear Regression\n",
    "    results['linear'] = train_linear_regression(X_train, y_train, X_test, y_test, target_name)\n",
    "    \n",
    "    # Model 2: Polynomial Regression\n",
    "    results['polynomial'] = train_polynomial_regression(X_train, y_train, X_test, y_test, target_name, degree=2)\n",
    "    \n",
    "    # Model 3: HP-Filtered\n",
    "    results['hp_filter'] = train_hp_filter_regression(X_train, y_train, X_test, y_test, target_name, lamb=1600)\n",
    "    \n",
    "    # Model 4: ARIMAX\n",
    "    results['arimax'] = train_arimax(X_train, y_train, X_test, y_test, target_name, \n",
    "                                      order=(1,0,1), seasonal_order=(0,0,1,4))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "def compare_models(results, target_name):\n",
    "    \"\"\"\n",
    "    Compare all models and create summary table.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL COMPARISON: {target_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, result in results.items():\n",
    "        if result is not None:  # Skip failed models\n",
    "            comparison_data.append({\n",
    "                'Model': model_name.replace('_', ' ').title(),\n",
    "                'Train R²': result['train_metrics']['R²'],\n",
    "                'Test R²': result['test_metrics']['R²'],\n",
    "                'Train RMSE': result['train_metrics']['RMSE'],\n",
    "                'Test RMSE': result['test_metrics']['RMSE'],\n",
    "                'Train MAE': result['train_metrics']['MAE'],\n",
    "                'Test MAE': result['test_metrics']['MAE'],\n",
    "                'Test MAPE': result['test_metrics']['MAPE']\n",
    "            })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_by_r2 = comparison_df.loc[comparison_df['Test R²'].idxmax(), 'Model']\n",
    "    best_model_by_rmse = comparison_df.loc[comparison_df['Test RMSE'].idxmin(), 'Model']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"BEST MODELS:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Highest Test R²: {best_model_by_r2} (R² = {comparison_df['Test R²'].max():.4f})\")\n",
    "    print(f\"  Lowest Test RMSE: {best_model_by_rmse} (RMSE = {comparison_df['Test RMSE'].min():.4f})\")\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "\n",
    "def plot_model_comparison(results, y_train, y_test, dates_train, dates_test, target_name):\n",
    "    \"\"\"\n",
    "    Create comprehensive comparison plots.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'Model Comparison: {target_name.upper()}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    model_names = ['linear', 'polynomial', 'hp_filter', 'arimax']\n",
    "    titles = ['Linear Regression', 'Polynomial Regression', 'HP-Filtered', 'ARIMAX']\n",
    "    \n",
    "    for idx, (model_key, title) in enumerate(zip(model_names, titles)):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        if model_key in results and results[model_key] is not None:\n",
    "            result = results[model_key]\n",
    "            \n",
    "            # Plot test set predictions\n",
    "            ax.plot(dates_test, y_test, label='Actual', color='black', linewidth=2)\n",
    "            ax.plot(dates_test, result['y_pred_test'], label='Predicted', \n",
    "                   color='red', linewidth=1.5, alpha=0.7)\n",
    "            ax.fill_between(dates_test, y_test, result['y_pred_test'], alpha=0.3, color='red')\n",
    "            \n",
    "            # Add metrics to plot\n",
    "            r2 = result['test_metrics']['R²']\n",
    "            rmse = result['test_metrics']['RMSE']\n",
    "            ax.text(0.02, 0.98, f'R² = {r2:.3f}\\nRMSE = {rmse:.3f}', \n",
    "                   transform=ax.transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "            \n",
    "            ax.set_title(title, fontweight='bold')\n",
    "            ax.set_xlabel('Date')\n",
    "            ax.set_ylabel(target_name)\n",
    "            ax.legend()\n",
    "            ax.grid(alpha=0.3)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'Model Failed', ha='center', va='center', \n",
    "                   transform=ax.transAxes, fontsize=14)\n",
    "            ax.set_title(title, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"PHASE 2: MODEL TRAINING & EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    targets = ['gdp_growth', 'inflation', 'unemployment']\n",
    "    all_results = {}\n",
    "    all_comparisons = {}\n",
    "    \n",
    "    for target in targets:\n",
    "        print(f\"\\n\\n{'#'*80}\")\n",
    "        print(f\"# PROCESSING TARGET: {target.upper()}\")\n",
    "        print(f\"{'#'*80}\")\n",
    "        \n",
    "        # Load data\n",
    "        X_train = cleaned_data[target]['X_train']\n",
    "        y_train = cleaned_data[target]['y_train']\n",
    "        X_test = cleaned_data[target]['X_test']\n",
    "        y_test = cleaned_data[target]['y_test']\n",
    "        dates_train = pd.read_csv(f'gen_files/train_{target}.csv')['Date']\n",
    "        dates_test = pd.read_csv(f'gen_files/test_{target}.csv')['Date']\n",
    "        \n",
    "        # Train all models\n",
    "        results = train_all_models(X_train, y_train, X_test, y_test, \n",
    "                                   dates_train, dates_test, target)\n",
    "        \n",
    "        # Compare models\n",
    "        comparison_df = compare_models(results, target)\n",
    "        \n",
    "        # Create comparison plots\n",
    "        fig = plot_model_comparison(results, y_train, y_test, \n",
    "                                    dates_train, dates_test, target)\n",
    "        fig.autofmt_xdate()\n",
    "        # fig.savefig(f'model_comparison_{target}.png', dpi=300, bbox_inches='tight')\n",
    "        # print(f\"\\n✓ Saved: model_comparison_{target}.png\")\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        # Store results\n",
    "        all_results[target] = results\n",
    "        all_comparisons[target] = comparison_df\n",
    "        \n",
    "        # # Save comparison table\n",
    "        # comparison_df.to_csv(f'model_comparison_{target}.csv', index=False)\n",
    "        # print(f\"✓ Saved: model_comparison_{target}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d854320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ea139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
